{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5fafc9",
   "metadata": {},
   "source": [
    "### Topic Names\n",
    "#### Topic 0: Privacy and Safety Concerns and Opinions on Camera Surveillance\n",
    "#### Topic 1: Personal Experiences and Directions\n",
    "#### Topic 2: Object Descriptions and Movement Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc95a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558f3db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg_type</th>\n",
       "      <th>datetime</th>\n",
       "      <th>title</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>topic_name_top</th>\n",
       "      <th>topic_name_leaf</th>\n",
       "      <th>thread_text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>topic</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thread_start</td>\n",
       "      <td>2001-01-18 17:18:00</td>\n",
       "      <td>Questions about Estonia</td>\n",
       "      <td>89742</td>\n",
       "      <td>0</td>\n",
       "      <td>Society</td>\n",
       "      <td># The world's going on #</td>\n",
       "      <td>- Why did an important piece of evidence (from...</td>\n",
       "      <td>important piece evidence front gate fall depth...</td>\n",
       "      <td>-1</td>\n",
       "      <td>important piece evidence front gate fall depth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>2001-01-19 01:28:00</td>\n",
       "      <td>Questions about Estonia</td>\n",
       "      <td>89742</td>\n",
       "      <td>386913</td>\n",
       "      <td>Society</td>\n",
       "      <td># The world's going on #</td>\n",
       "      <td>I think they had other things to do. - First o...</td>\n",
       "      <td>think thing first kin demanded sepulchre would...</td>\n",
       "      <td>-1</td>\n",
       "      <td>think thing first kin demanded sepulchre would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>2001-01-19 06:21:00</td>\n",
       "      <td>Questions about Estonia</td>\n",
       "      <td>89742</td>\n",
       "      <td>386914</td>\n",
       "      <td>Society</td>\n",
       "      <td># The world's going on #</td>\n",
       "      <td>- Not the bow gate, but its SARANA- &amp;gt; a str...</td>\n",
       "      <td>bow gate sarana strange coincidence fell resea...</td>\n",
       "      <td>-1</td>\n",
       "      <td>bow gate sarana gt strange coincidence fell re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comment</td>\n",
       "      <td>2001-01-19 16:27:00</td>\n",
       "      <td>Questions about Estonia</td>\n",
       "      <td>89742</td>\n",
       "      <td>386915</td>\n",
       "      <td>Society</td>\n",
       "      <td># The world's going on #</td>\n",
       "      <td>I'll tell you what's going on. I'll tell you w...</td>\n",
       "      <td>ill tell whats going ill tell whatll news</td>\n",
       "      <td>350</td>\n",
       "      <td>ill tell whats going ill tell whatll news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>2001-01-20 12:49:00</td>\n",
       "      <td>Questions about Estonia</td>\n",
       "      <td>89742</td>\n",
       "      <td>386916</td>\n",
       "      <td>Society</td>\n",
       "      <td># The world's going on #</td>\n",
       "      <td>Let the cause of the sinking of any ship be li...</td>\n",
       "      <td>let cause sinking ship lifted away wreckage le...</td>\n",
       "      <td>-1</td>\n",
       "      <td>let cause sinking ship lifted away wreckage le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       msg_type             datetime                    title  thread_id  \\\n",
       "0  thread_start  2001-01-18 17:18:00  Questions about Estonia      89742   \n",
       "1       comment  2001-01-19 01:28:00  Questions about Estonia      89742   \n",
       "2       comment  2001-01-19 06:21:00  Questions about Estonia      89742   \n",
       "3       comment  2001-01-19 16:27:00  Questions about Estonia      89742   \n",
       "4       comment  2001-01-20 12:49:00  Questions about Estonia      89742   \n",
       "\n",
       "   comment_id topic_name_top           topic_name_leaf  \\\n",
       "0           0        Society  # The world's going on #   \n",
       "1      386913        Society  # The world's going on #   \n",
       "2      386914        Society  # The world's going on #   \n",
       "3      386915        Society  # The world's going on #   \n",
       "4      386916        Society  # The world's going on #   \n",
       "\n",
       "                                         thread_text  \\\n",
       "0  - Why did an important piece of evidence (from...   \n",
       "1  I think they had other things to do. - First o...   \n",
       "2  - Not the bow gate, but its SARANA- &gt; a str...   \n",
       "3  I'll tell you what's going on. I'll tell you w...   \n",
       "4  Let the cause of the sinking of any ship be li...   \n",
       "\n",
       "                                      processed_text  topic  \\\n",
       "0  important piece evidence front gate fall depth...     -1   \n",
       "1  think thing first kin demanded sepulchre would...     -1   \n",
       "2  bow gate sarana strange coincidence fell resea...     -1   \n",
       "3          ill tell whats going ill tell whatll news    350   \n",
       "4  let cause sinking ship lifted away wreckage le...     -1   \n",
       "\n",
       "                                         probability  \n",
       "0  important piece evidence front gate fall depth...  \n",
       "1  think thing first kin demanded sepulchre would...  \n",
       "2  bow gate sarana gt strange coincidence fell re...  \n",
       "3          ill tell whats going ill tell whatll news  \n",
       "4  let cause sinking ship lifted away wreckage le...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'C:/Users/fahad/OneDrive - Oulun yliopisto/Documents/suomi24/Data/suomi24.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e09238fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lexicon\u001b[38;5;241m.\u001b[39manalyze(text, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Apply the function to each topic\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m empath_results \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extract_empath_categories)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Convert the results to a DataFrame\u001b[39;00m\n\u001b[0;32m     12\u001b[0m empath_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(empath_results))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4759\u001b[0m         func,\n\u001b[0;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1291\u001b[0m )\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mextract_empath_categories\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_empath_categories\u001b[39m(text):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lexicon\u001b[38;5;241m.\u001b[39manalyze(text, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\empath\\core.py:46\u001b[0m, in \u001b[0;36mEmpath.analyze\u001b[1;34m(self, doc, categories, tokenizer, normalize)\u001b[0m\n\u001b[0;32m     44\u001b[0m invcats \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m categories:\n\u001b[1;32m---> 46\u001b[0m    \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcats[k]: invcats[t]\u001b[38;5;241m.\u001b[39mappend(k)\n\u001b[0;32m     47\u001b[0m count \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     48\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize Empath\n",
    "lexicon = Empath()\n",
    "\n",
    "# Define a function to extract empath categories\n",
    "def extract_empath_categories(text):\n",
    "    return lexicon.analyze(text, normalize=True)\n",
    "\n",
    "# Apply the function to each topic\n",
    "empath_results = df['processed_text'].apply(extract_empath_categories)\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "empath_df = pd.DataFrame(list(empath_results))\n",
    "empath_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152df36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to generate a word cloud\n",
    "def generate_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate word clouds for each topic\n",
    "for topic_id in df['new_topics_merged'].unique():\n",
    "    if topic_id != -1:  # Skip outliers\n",
    "        topic_texts = ' '.join(df[df['new_topics_merged'] == topic_id]['processed_text'])\n",
    "        generate_wordcloud(topic_texts, f\"Topic {topic_id} Word Cloud\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bec9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sum the empath categories for each topic\n",
    "empath_sums = empath_df.groupby(df['new_topics_merged']).sum()\n",
    "\n",
    "# Plot the empath categories for each topic\n",
    "for topic_id in empath_sums.index:\n",
    "    if topic_id != -1:  # Skip outliers\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x=empath_sums.columns, y=empath_sums.loc[topic_id].values)\n",
    "        plt.title(f\"Empath Categories for Topic {topic_id}\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c351e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and topic\n",
    "df['year'] = df['datetime'].dt.year\n",
    "empath_yearly = pd.concat([df['year'], empath_df], axis=1)\n",
    "empath_yearly = empath_yearly.groupby(['year', df['new_topics_merged']]).mean()\n",
    "\n",
    "# Plot the empath categories over time for each topic\n",
    "for topic_id in df['new_topics_merged'].unique():\n",
    "    if topic_id != -1:  # Skip outliers\n",
    "        topic_trend = empath_yearly.xs(topic_id, level=1)\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        sns.lineplot(data=topic_trend)\n",
    "        plt.title(f\"Empath Categories Over Time for Topic {topic_id}\")\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Average Category Score')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
